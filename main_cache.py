import streamlit as st
from langsmith import uuid7

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# custom tools
from tools.fetch_qa_content import fetch_qa_content
from tools.fetch_stores_by_prefecture import fetch_stores_by_prefecture
from src.cache import Cache


@st.cache_data
def load_system_prompt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def init_page():
    st.set_page_config(page_title="ê³ ê° ì§€ì›", page_icon="ğŸ»")
    st.header("ê³ ê° ì§€ì›ğŸ»")
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”", key="clear")
    if clear_button or "messages" not in st.session_state:
        welcome_message = (
            "ì˜ì§„ëª¨ë°”ì¼ ê³ ê°ì§€ì›ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ»"
        )
        st.session_state.messages = [{"role": "assistant", "content": welcome_message}]
        st.session_state["checkpointer"] = InMemorySaver()
        st.session_state["thread_id"] = str(uuid7())

    if len(st.session_state.messages) == 1:
        st.session_state["first_question"] = True
    else:
        st.session_state["first_question"] = False


def select_model(temperature=0):
    models = ("GPT-5 mini", "GPT-5.2", "Claude Sonnet 4.5", "Gemini 2.5 Flash")
    model = st.sidebar.radio("ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ:", models)
    if model == "GPT-5 mini":
        return ChatOpenAI(temperature=temperature, model="gpt-5-mini")
    elif model == "GPT-5.2":
        return ChatOpenAI(temperature=temperature, model="gpt-5.2")
    elif model == "Claude Sonnet 4.5":
        return ChatAnthropic(
            temperature=temperature, model="claude-sonnet-4-5-20250929"
        )
    elif model == "Gemini 2.5 Flash":
        return ChatGoogleGenerativeAI(temperature=temperature, model="gemini-2.5-flash")


def create_customer_support_agent():
    tools = [fetch_qa_content, fetch_stores_by_prefecture]
    custom_system_prompt = load_system_prompt("./prompt/system_prompt.txt")
    llm = select_model()

    summarization_middleware = SummarizationMiddleware(
        model=llm,
        max_tokens_before_summary=8000,
        messages_to_keep=10,
    )

    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt=custom_system_prompt,
        checkpointer=st.session_state["checkpointer"],
        middleware=[summarization_middleware],
        debug=True,
    )

    return agent


def main():
    init_page()
    init_messages()
    customer_support_agent = create_customer_support_agent()

    # ìºì‹œ ì´ˆê¸°í™”
    cache = Cache()
    config = {"configurable": {"thread_id": st.session_state["thread_id"]}}

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input(placeholder="ë²•ì¸ ëª…ì˜ë¡œ ê³„ì•½ì´ ê°€ëŠ¥í•œê°€ìš”?"):
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ ê²½ìš° ìºì‹œ í™•ì¸
        if st.session_state["first_question"]:
            if cache_content := cache.search(query=prompt):
                st.chat_message("assistant").write(f"(cache) {cache_content}")
                st.session_state.messages.append(
                    {"role": "assistant", "content": cache_content}
                )
                st.stop()  # ìºì‹œ ë‚´ìš©ì„ ì¶œë ¥í•œ ê²½ìš° ì‹¤í–‰ ì¢…ë£Œ

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                result = customer_support_agent.invoke(
                    {"messages": [{"role": "user", "content": prompt}]}, config
                )
            response = result["messages"][-1].content
            st.write(response)

            if response:
                st.session_state.messages.append(
                    {"role": "assistant", "content": response}
                )

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ì¸ ê²½ìš° ìºì‹œì— ì €ì¥
        if st.session_state["first_question"] and response:
            cache.save(prompt, response)


if __name__ == "__main__":
    main()
